@inproceedings{alkhouli2016alignment,
    title={Alignment-based neural machine translation},
    author={Alkhouli, Tamer and Bretschner, Gabriel and Peter, Jan-Thorsten and Hethnawi, Mohammed and Guta, Andreas and Ney, Hermann},
    booktitle={Proceedings of the First Conference on Machine Translation: Volume 1, Research Papers},
    pages={54--65},
    year={2016}
}

@inproceedings{schrader2006does,
    title={How does morphological complexity translate? A cross-linguistic case study for word alignment},
    author={Schrader, Bettina},
    booktitle={Proceedings of Linguistic Evidence Conference},
    pages={189--191},
    year={2006}
}

@inproceedings{specia2013quest,
    title={QuEst-A translation quality estimation framework},
    author={Specia, Lucia and Shah, Kashif and De Souza, Jos{\'e} GC and Cohn, Trevor},
    booktitle={Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics: System Demonstrations},
    pages={79--84},
    year={2013}
}

% Possibly change this to later publication
@article{zouhar2020extending,
    title={Extending {P}takop{\v{e}}t for Machine Translation User Interaction Experiments},
    author={Zouhar, Vil{\'e}m and Nov{\'a}k, Michal},
    journal={The Prague Bulletin of Mathematical Linguistics},
    number={115},
    pages={129--142},
    year={2020}
}

@inproceedings{mihalcea2003evaluation,
    title={An evaluation exercise for word alignment},
    author={Mihalcea, Rada and Pedersen, Ted},
    booktitle={Proceedings of the HLT-NAACL 2003 Workshop on Building and using parallel texts: data driven machine translation and beyond},
    pages={1--10},
    year={2003}
}

@inproceedings{dyer2013simple,
    title={A simple, fast, and effective reparameterization of ibm model 2},
    author={Dyer, Chris and Chahuneau, Victor and Smith, Noah A},
    booktitle={Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
    pages={644--648},
    year={2013}
}

@article{junczys2018marian,
    title={Marian: Fast neural machine translation in {C}++},
    author={Junczys-Dowmunt, Marcin and Grundkiewicz, Roman and Dwojak, Tomasz and Hoang, Hieu and Heafield, Kenneth and Neckermann, Tom and Seide, Frank and Germann, Ulrich and Aji, Alham Fikri and Bogoychev, Nikolay and others},
    journal={arXiv preprint arXiv:1804.00344},
    year={2018}
}

@article{zintgraf2017visualizing,
    title={Visualizing deep neural network decisions: Prediction difference analysis},
    author={Zintgraf, Luisa M and Cohen, Taco S and Adel, Tameem and Welling, Max},
    journal={arXiv preprint arXiv:1702.04595},
    year={2017}
}

@inproceedings{li2019word,
    title={On the word alignment from neural machine translation},
    author={Li, Xintong and Li, Guanlin and Liu, Lemao and Meng, Max and Shi, Shuming},
    booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
    pages={1293--1303},
    year={2019}
}

@InProceedings{germann-EtAl:2020:WMT,
    author    = {Germann, Ulrich  and  Grundkiewicz, Roman  and  Popel, Martin  and  Dobreva, Radina  and  Bogoychev, Nikolay  and  Heafield, Kenneth},
    title     = {Speed-optimized, Compact Student Models that Distill Knowledge from a Larger Teacher Model: the UEDIN-CUNI Submission to the WMT 2020 News Translation Task},
    year           = {2020},
    address        = {Online},
    publisher      = {Association for Computational Linguistics},
    pages     = {190--195},
}

@misc{vaswani2017transformer,
    title={Attention Is All You Need}, 
    author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
    year={2017},
    eprint={1706.03762},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@book{koehn2009statistical,
    title={Statistical machine translation},
    author={Koehn, Philipp},
    year={2009},
    publisher={Cambridge University Press}
}

@misc{marecek_csen_algn_corpus,
    title = {Czech-English Manual Word Alignment},
    author = {Mare{\v c}ek, David},
    url = {http://hdl.handle.net/11234/1-1804},
    note = {{LINDAT}/{CLARIAH}-{CZ} digital library at the Institute of Formal and Applied Linguistics ({{\'U}FAL}), Faculty of Mathematics and Physics, Charles University},
    copyright = {Creative Commons - Attribution-{ShareAlike} 4.0 International ({CC} {BY}-{SA} 4.0)},
    year = {2016}
}

@phdthesis{bicini_ende_algn_corpus,
    author = {Ergun Bi\c{c}ici},
    note = {Supervisor: Deniz Yuret},
    title = {The Regression Model of Machine Translation},
    year = {2011},
    school = {Ko\c{c} University},
    keywords = "Machine Translation, Machine Learning, Artificial Intelligence, Natural Language Processing",
    pdf = "http://home.ku.edu.tr/~ebicici/publications/2011/RegMTThesis_Web.pdf",
}

@inproceedings{rozis_tilde,
  title={Tilde MODEL-multilingual open data for EU languages},
  author={Rozis, Roberts and Skadi{\c{n}}{\v{s}}, Raivis},
  booktitle={Proceedings of the 21st Nordic Conference on Computational Linguistics},
  pages={263--265},
  year={2017}
}

@inproceedings{sacrebleu,
    title = "A Call for Clarity in Reporting {BLEU} Scores",
    author = "Post, Matt",
    booktitle = "Proceedings of the Third Conference on Machine Translation: Research Papers",
    month = oct,
    year = "2018",
    address = "Belgium, Brussels",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-6319",
    pages = "186--191",
}

@article{kudo2018sentencepiece,
  title={Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing},
  author={Kudo, Taku and Richardson, John},
  journal={arXiv preprint arXiv:1808.06226},
  year={2018}
}

@inproceedings{model_csen,
  author = {Germann, Ulrich and Grundkiewicz, Roman and Popel, Martin and Dobreva, Radina and Bogoychev, Nikolay and Heafield, Kenneth},
  title = {Speed-optimized, Compact Student Models that Distill Knowledge from a Larger Teacher Model: the UEDIN-CUNI Submission to the WMT 2020 News Translation Task},
  booktitle = {Proceedings of the Fifth Conference on Machine Translation},
  month = nov,
  year = {2020},
  address = {Online},
  publisher = {Association for Computational Linguistics},
  pages = {190--195},
  abstract = {We describe the joint submission of the University of Edinburgh and Charles University, Prague, to the Czech/English track in the WMT 2020 Shared Task on News Translation. Our fast and compact student models distill knowledge from a larger, slower teacher. They are designed to offer a good trade-off between translation quality and inference efficiency. On the WMT 2020 Czech â†” English test sets, they achieve translation speeds of over 700 whitespace-delimited source words per second on a single CPU thread, thus making neural translation feasible on consumer hardware without a GPU.},
  url = {https://www.aclweb.org/anthology/2020.wmt-1.17},
  month_numeric = {11}
}

@inproceedings{model_deen,
  title = {{E}dinburgh{'}s Submissions to the 2020 Machine Translation Efficiency Task},
  author = {Bogoychev, Nikolay and Grundkiewicz, Roman and Aji, Alham Fikri and Behnke, Maximiliana and Heafield, Kenneth and Kashyap, Sidharth and Farsarakis, Emmanouil-Ioannis and Chudyk, Mateusz},
  booktitle = {Proceedings of the Fourth Workshop on Neural Generation and Translation},
  month = jul,
  year = {2020},
  address = {Online},
  publisher = {Association for Computational Linguistics},
  url = {https://www.aclweb.org/anthology/2020.ngt-1.26},
  pages = {218--224},
  abstract = {We participated in all tracks of the Workshop on Neural Generation and Translation 2020 Efficiency Shared Task: single-core CPU, multi-core CPU, and GPU. At the model level, we use teacher-student training with a variety of student sizes, tie embeddings and sometimes layers, use the Simpler Simple Recurrent Unit, and introduce head pruning. On GPUs, we used 16-bit floating-point tensor cores. On CPUs, we customized 8-bit quantization and multiple processes with affinity for the multi-core setting. To reduce model size, we experimented with 4-bit log quantization but use floats at runtime. In the shared task, most of our submissions were Pareto optimal with respect the trade-off between time and quality.},
  month_numeric = {7}
}
